## 第三章 算法设计

### 3.1 算法总体框架与设计动机

针对第二章建立的空天地一体化网络（SAGIN）多无人机协同边缘计算模型，本章提出一种基于注意力机制增强的多智能体近端策略优化（Attention-based Multi-Agent Proximal Policy Optimization, Attention-based MAPPO）算法。该算法旨在高动态、强约束的 SAGIBasN 环境中，求解具有混合动作空间和变长观测输入的 Dec-POMDP 问题。

#### 3.1.1 框架选择

针对本研究定义的 Dec-POMDP 模型 $\mathcal{M} = \langle \mathcal{N}, \mathcal{S}, \mathcal{O}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$，选择 MAPPO 作为核心算法框架，并结合注意力机制进行架构重构。该选择主要基于以下三方面的考量：

**1. 耦合混合动作空间适配**

本系统模型中的联合动作空间 $\mathcal{A}$ 表现出显著的离散-连续耦合特性：
*   **连续域控制：** 无人机的机动控制 $\hat{\mathbf{a}}_{acc} \in [-1, 1]^2$ 与带宽分配权重 $\mathbf{z}_{bw} \in \mathbb{R}^{K_{max}}$ 要求策略输出高精度的连续数值，以保障飞行轨迹的平滑性与资源分配的精细度。
*   **离散域决策：** 卫星关联决策 $\boldsymbol{\zeta}_{u}$ 本质上是在可见卫星集合 $\mathcal{O}_{u}^{sats}$ 中的类别选择问题。

在现有的多智能体强化学习算法中，Q-Mix等值分解类算法严格受限于离散动作空间，若对带宽分配进行离散化处理，将导致动作空间维度呈指数级爆炸，严重影响收敛性能。另一方面，MADDPG等确定性策略梯度算法虽支持连续动作，但处理离散分支时需引入 Gumbel-Softmax 等松弛技巧，且其确定性策略在 SAGIN 这种拓扑剧变的随机环境中，极易陷入局部最优。
相比之下，PPO（Proximal Policy Optimization）通过多头网络架构，能够同时输出用于连续控制的高斯分布参数和用于离散选择的类别分布概率。其随机策略特性提供了更强的环境探索能力，有效避免了算法在动态拓扑变化中陷入循环死锁。

**2. 集中式训练-分布式执行（CTDE）架构的必要性**

针对多无人机并发学习引起的环境非平稳性问题，本算法严格遵循集中式训练-分布式执行（CTDE）范式：

*   **集中式训练：**
    引入中心化的评论家网络（Critic Network），其输入为全局状态 $s_t = \{ \mathbf{S}_{UAV}, \mathbf{S}_{GU}, \mathbf{S}_{SAT} \}$。该状态包含了全网节点的精确位置、速度矢量及完整的任务队列积压（Backlog）信息。Critic 利用这一全局视角拟合状态价值函数 $V_{\phi}(s_t)$，从而准确评估当前联合动作对全网长期累积回报（如总时延与能耗加权和）的边际贡献，有效降低了多智能体策略梯度的方差。

*   **分布式执行：**
    每个 UAV 部署独立的演员网络（Actor Network），其策略 $\pi_{\theta}(a_u|o_u)$ 仅依赖于局部观测 $o_u(t)$ 进行推理。这严格符合 SAGIN 的物理约束：UAV在工作过程中无法维持与其他节点的全状态实时同步，必须基于自身状态和有限的邻居信息进行独立决策。

**3. 动态拓扑与变长输入的表征学习**

传统的 RL 算法通常使用多层感知机（MLP）或卷积神经网络（CNN）处理观测数据，这要求输入特征具有固定的维度。然而，在本系统的局部观测 $o_u(t)$ 中，候选用户集合 $\mathcal{O}_{u}^{users}$ 和可见卫星集合 $\mathcal{O}_{u}^{sats}$ 具有双重动态性：
*   **时变性：** 随着卫星过顶和 UAV 移动，可视节点的数量随时间 $t$ 剧烈波动。
*   **密度差异：** 不同区域的 UAV 覆盖的用户数量存在数量级差异（如热点区域与偏远区域间）。

直接使用零填充（Zero-padding）或最大池化（Max-pooling）会导致信息丢失或引入噪声。为此，我们在 Actor 和 Critic 网络的前端引入注意力机制（Attention）。该机制利用其置换不变性和变长序列处理能力，将无序、变长的邻居节点集合映射为固定维度的特征向量（Embedding）。更重要的是，注意力权重能够自适应地筛选关键节点（例如：优先关注积压严重的用户或能量充足的卫星），从而显著提升算法在动态拓扑下的泛化能力。

综上所述，Attention-based MAPPO 算法在处理混合动作空间、解决多智能体协作不稳定性以及应对动态拓扑输入方面具有显著优势。算法的优化目标是在满足约束的前提下，最大化如下代理目标函数：

$$
J(\theta) = \mathbb{E}_{t} \left[ \frac{1}{N_u} \sum_{u=1}^{N_u} \min \left( r_t^{(u)}(\theta) \hat{A}_t^{(u)}, \text{clip}(r_t^{(u)}(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t^{(u)} \right) \right]
$$

其中，$N_u$ 为无人机数量，$r_t^{(u)}(\theta) = \frac{\pi_{\theta}(a_u|o_u)}{\pi_{\theta_{old}}(a_u|o_u)}$ 为新旧策略的概率比率，$\hat{A}_t^{(u)}$ 为基于全局状态价值 $V_{\phi}(s_t)$ 计算的广义优势估计（GAE），$\epsilon$ 为防止策略更新步长过大的裁剪系数。

### 3.1.2 网络架构设计

鉴于 SAGIN 环境的高度动态性与空天地节点的异构性，传统的固定维度全连接网络难以有效表征变长的局部观测与全局状态。对于Actor 网络，简单的零填充不仅引入稀疏计算冗余，更无法捕捉节点集合的置换不变性，难以处理 UAV 视角下的动态优先级问题；对于Critic网络，直接对全局节点特征进行简单聚合会丢失关键的拓扑分布信息（如用户热点分布、卫星星座构型）。

为此，本研究分别构建了不同的注意力增强架构。Actor网络采用多头目标注意力（Target-Attention）实现UAV局部信息的感知；Critic网络采用多头自注意力（Self-Attention）结合全局池化（Global Pooling）实现对全局信息的感知与评估。

#### 1. 分布式 Actor 网络：基于目标注意力的局部感知

Actor 网络部署于每个 UAV，负责将变长的局部观测 $o_u(t)$ 映射为动作策略。观测数据包含变长的候选用户集合 $\mathcal{K}_u(t)$、可见卫星集合 $\mathcal{O}_{u}^{sats}(t)$ 及邻居无人机集合 $\mathcal{N}_u(t)$。

为了使 UAV 能够根据自身状态动态筛选关键环境信息，采用目标注意力机制。设 UAV 自身的本体特征嵌入为 $\mathbf{e}_{u}^{own}$（作为 Query），环境实体集合的特征矩阵为 $\mathbf{X}_{env}$（作为 Key 和 Value）。

*   **多头注意力计算：**
    引入 $H$ 个并行注意力头以捕捉不同子空间的相关性。对于第 $h$ 个头，注意力权重 $\boldsymbol{\alpha}_h$ 计算如下：
    $$
    \mathbf{q}_h = \mathbf{e}_{u}^{own} \mathbf{W}_Q^h, \quad \mathbf{K}_h = \mathbf{X}_{env} \mathbf{W}_K^h
    $$
    $$
    \boldsymbol{\alpha}_h = \text{softmax}\left( \frac{\mathbf{q}_h \mathbf{K}_h^T}{\sqrt{d_k}} \right)
    $$
    该机制使 UAV 能够自适应地赋予高优先级节点更大的权重（例如：关注积压严重的用户或 SNR 高的卫星），生成各组实体的上下文向量 $\mathbf{c}_{u}^{users}, \mathbf{c}_{u}^{sats}, \mathbf{c}_{u}^{nbrs}$。

*   **特定编码器设计：**
    *   **用户特征编码器：** 该模块旨在从候选用户集合 $\mathcal{K}_u(t)$ 中提取对当前卸载决策至关重要的聚合特征。
        *   **输入：** 用户集合特征 $\Phi_{gu} = \{ \boldsymbol{\phi}_{k}^{gu} \}_{k \in \mathcal{K}_u(t)}$，其中 $\boldsymbol{\phi}_{k}^{gu}$ 包含用户相对位置、积压队列 $Q_k^{gu}(t)$、频谱效率 $\eta_{k,u}(t)$ 及关联历史特征。
        *   **处理：** 将 $\Phi_{gu}$ 映射为 Key/Value，UAV 自身特征 $\mathbf{e}_{u}^{own}$ 映射为 Query。
        *   通过端到端训练，网络将自动学习调整权重 $\alpha_{k}^{gu}$。例如，当 UAV 计算资源空闲时，权重将向积压量 $Q_k$ 较大的用户倾斜；当 UAV 能量不足时，权重可能向传输能耗较低的用户倾斜。最终输出固定维度的用户上下文向量 $\mathbf{c}_{u}^{users}$。
    *   **卫星特征编码器：** 该模块处理可见卫星特征集合 $\Phi_{sat} = \{ \boldsymbol{\phi}_{l}^{sat} \}_{l \in \mathcal{O}_{u}^{sats}(t)}$，生成回程链路上下文向量 $\mathbf{c}_{u}^{sats}$。其结构与用户编码器相同，但在特征交互上具有特殊的物理约束：
        *   **输入特征：** 包含星机相对位置、相对速度、积压队列$Q_k^{comp}(t)$、有效信噪比 $\text{SNR}_{u,l}^{eff}$ 以及相对于 UAV 的多普勒频移 $\nu_{u,l}$。
        *   **注意力掩码（Soft-Masking）：** 针对高速运动导致的快衰落特性，网络通过学习会对 $\nu_{u,l}$ 接近最大频移阈值或 SNR 过低的卫星赋予极低的注意力权重（$\alpha_{l}^{sat} \to 0$），从而指引 Actor 在后续决策中有效地屏蔽不可靠的卫星链路。
    *   **邻居无人机编码器**
        为了实现多机协同并避免碰撞，UAV 需感知通信范围内的邻居节点 $\mathcal{O}_{u}^{nbrs}(t)$。
        *   **机制：** 同样采用 Target-Attention，将邻居集合特征 $\Phi_{nbr} = \{ \boldsymbol{\phi}_{u'}^{nbr} \}_{u' \in \mathcal{O}_{u}^{nbrs}(t)}$ 映射为上下文向量 $\mathbf{c}_{u}^{nbrs}$。
        *   **作用：** 该特征向量主要编码了邻居的相对位置（用于防碰撞），帮助智能体在密集场景下做出协调一致的动作。

*   **时序决策生成：**
    将上述上下文向量与自身特征拼接，输入门控循环单元（GRU）以聚合历史信息：
    $$
    \mathbf{h}_{actor}(t) = \text{GRU}\left( \text{Concat}[\mathbf{e}_{u}^{own}, \mathbf{c}_{u}^{users}, \mathbf{c}_{u}^{sats}, \mathbf{c}_{u}^{nbrs}], \mathbf{h}_{actor}(t-1) \right)
    $$
    最终，$\mathbf{h}_{actor}(t)$ 分别输入动作头，输出机动（连续高斯分布）、带宽（Softmax分布）及卫星关联（Masked Logits）策略。

#### 2. 集中式 Critic 网络：基于自注意力与全局池化的整体评估

Critic 网络位于中心服务器，输入为全局系统状态 $s(t)$，包含全网所有用户状态 **$\mathbf{S}_{GU}$**、卫星状态 **$\mathbf{S}_{SAT}$** 及 UAV 状态 **$\mathbf{S}_{UAV}$**（对应集合 $\mathcal{K}, \mathcal{L}, \mathcal{U}$）的完整信息。
与 Actor 不同，Critic不存在单一的 Query，而是从变长节点集合中提取反映系统整体价值的全局描述符。

*   **多头自注意力层：**
    为了解决简单池化导致结构信息丢失的问题，在聚合前引入自注意力机制。以全网用户集合 $\mathbf{S}_{GU}$ 为例，让用户节点之间进行特征交互：
    $$
    \mathbf{H}_{users} = \text{MHSA}(\mathbf{Q}=\mathbf{S}_{GU}, \mathbf{K}=\mathbf{S}_{GU}, \mathbf{V}=\mathbf{S}_{GU})
    $$
    通过自注意力，每个用户的特征向量被更新为包含了其局部邻域上下文（如“是否位于热点区域”）的新特征。这一步确保了拓扑结构被编码进节点特征中。

*   **全局平均池化：**
    在完成拓扑特征提取后，为了将变长集合映射为固定维度的全局向量，采用全局平均池化：
    $$
    \mathbf{g}_{users} = \frac{1}{K} \sum_{i=1}^{K} \mathbf{H}_{users}[i, :]
    $$
    此时的 $\mathbf{g}_{users}$ 不再只是简单的平均，而是反映全网负载和拥塞程度的全局特征。

*   **全局价值拟合：**
    将三类实体的全局描述符拼接：$\mathbf{x}_{critic} = [\mathbf{g}_{users}, \mathbf{g}_{sats}, \mathbf{g}_{uavs}]$，送入 GRU 网络捕捉系统状态演变，最终通过 MLP 输出标量价值 $V(s_t)$。该设计既解决了输入变长问题，又最大限度保留了影响全网性能的关键分布特征。

## 3.2 约束感知与动作掩码

在空天地一体化网络中，智能体的动作空间受到严格的物理与资源约束。若直接采用标准的无约束强化学习策略，智能体在初期探索阶段极易产生大量无效动作（例如试图连接地平线以下的卫星或分配带宽给非视距用户），这不仅会导致严重的性能惩罚（如通信中断），还会导致策略梯度在无效空间震荡，显著降低训练收敛速度。
为此，本节设计了一种确定性的约束感知机制，将物理约束数学化为神经网络输出层的动作掩码，确保智能体仅在可行域内进行探索。

### 3.2.1 离散动作屏蔽：多普勒容限与卫星关联

针对动作空间中的离散决策维度——卫星关联变量 $\boldsymbol{\zeta}_{u}(t)$，系统模型中存在两个关键的物理硬约束（对应约束 C3, C4）：
1.  **几何可见性：** 卫星必须位于 UAV 的观测仰角阈值 $\theta_{min}$ 以上。
2.  **链路可靠性：** 相对运动产生的多普勒频移 $\nu_{u,l}(t)$ 必须小于接收机锁相环的捕获范围（即最大容限 $\nu_{max}$），否则即使信噪比满足要求，链路也无法建立同步。

为了在策略生成阶段强制满足上述约束，我们在 Actor 网络输出卫星选择概率之前，引入动态掩码向量 $\mathbf{M}_{sat}(t)$。

**1. 动态掩码向量的构建**

设 Actor 网络关于卫星关联的原始输出 Logits 向量为 $\mathbf{z}_{sat}(t) \in \mathbb{R}^{|\mathcal{O}_{u}^{sats}|}$。根据当前时刻的局部观测 $o_u(t)$，构建同维度的掩码向量 $\mathbf{M}_{sat}(t)$。对于第 $l$ 颗可见卫星，其掩码值 $m_{l}(t)$ 定义如下：

$$
m_{l}(t) = \begin{cases} 
0, & \text{if } \underbrace{\theta_{u,l}(t) \ge \theta_{min}}_{\text{可见性约束}} \text{ AND } \underbrace{|\nu_{u,l}(t)| \le \nu_{max}}_{\text{多普勒约束}} \\
-\infty, & \text{otherwise}
\end{cases}
$$

其中，$\theta_{u,l}(t)$ 为卫星仰角，$\nu_{u,l}(t)$ 为多普勒频移值。

**2. 掩码操作与概率分布生成**

将掩码向量叠加至原始 Logits 上，得到修正后的 Logits 向量 $\tilde{\mathbf{z}}_{sat}(t)$：

$$
\tilde{\mathbf{z}}_{sat}(t) = \mathbf{z}_{sat}(t) + \mathbf{M}_{sat}(t)
$$

随后，利用 Softmax 函数生成合法的关联概率分布 $\pi_{sat}(\cdot | o_u)$：

$$
\pi_{sat}(l | o_u) = \text{Softmax}(\tilde{\mathbf{z}}_{sat}(t))_l = \frac{e^{z_{l} + m_{l}}}{\sum_{j \in \mathcal{O}_{u}^{sats}} e^{z_{j} + m_{j}}}
$$

由于当 $m_j = -\infty$ 时，$e^{z_j + m_j} \to 0$，该机制从数学上保证了不可用卫星被采样的概率严格为 0。这不仅避免了无效链路尝试带来的连接失败惩罚，还使得策略梯度仅在有效动作空间内传播，大幅提升了学习效率。

**3. 射频资源约束处理 ($N_{RF}$)**

考虑到 UAV 配备的射频链路数量有限（$N_{RF} < |\mathcal{O}_{u}^{sats}|$），智能体需从可行卫星集合中选择至多 $N_{RF}$ 个目标建立连接。为了保证训练阶段的探索性与执行阶段的性能最优性，设计了分阶段采样策略：

*   **训练阶段：无放回加权采样**
    为了保持策略的随机性以充分探索解空间，依据概率分布 $\pi_{sat}$ 进行无放回加权采样（Weighted Sampling without Replacement）。即在每次采样后，将已选卫星的概率置为 0 并重新归一化，直至选出 $N_{RF}$ 个卫星或所有可行卫星被选尽。这种方式能防止智能体过早陷入次优的连接拓扑。

*   **执行阶段：Top-K 确定性选择**
    在模型部署或性能评估阶段，为了最大化系统收益，采用Top-K 贪婪策略。即直接选择概率值 $\pi_{sat}$ 最大的前 $N_{RF}$ 个有效卫星建立连接。若当前满足多普勒约束的有效卫星数量 $N_{valid} < N_{RF}$，则仅建立 $N_{valid}$ 条链路，剩余射频端口保持空闲，以此严格满足资源上限约束。

### 3.2.2 连续动作修正：机动控制与能量安全层

不同于卫星关联的离散选择，无人机的机动控制（速度矢量）与带宽分配属于连续动作空间。标准的 PPO 算法通常假设动作服从高斯分布 $\mathbf{a} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma})$，其采样值在数值上是无界的。若直接将原始输出应用于控制系统，将导致速度超限或非法的资源分配。为此，本节设计了基于运动学模型的动作投影与基于 U 型功率曲线的能量安全层。

**1.  运动学约束与速度更新逻辑**

为了满足 UAV 最大飞行速度 $V_{max}$ 和机间防碰撞安全距离的限制（对应系统模型约束 C1），设计了如下修正逻辑。

*   **阶段一：人工势场避障修正**
    设 Actor 网络输出的原始期望加速度为 $\hat{\mathbf{a}}_{u}(t) \in \mathbb{R}^2$。首先，利用观测中的邻居集合 $\mathcal{N}_u(t)$ 检查碰撞风险。定义避障警戒距离 $d_{alert} = \gamma \cdot d_{safe}$（其中 $\gamma > 1$ 为安全缓冲系数）。若 UAV $u$ 与邻居 $j$ 的距离 $d_{uj} < d_{alert}$，则产生斥力加速度：
    $$
    \mathbf{a}_{rep}^{uj}(t) = \eta \cdot \left( \frac{1}{d_{uj}} - \frac{1}{d_{alert}} \right) \cdot \frac{\mathbf{q}_u - \mathbf{q}_j}{d_{uj}}
    $$
    其中 $\eta$ 为斥力增益系数。修正后的中间态加速度为 $\tilde{\mathbf{a}}_{u}(t) = \hat{\mathbf{a}}_{u}(t) + \sum_{j \in \mathcal{N}_u} \mathbf{a}_{rep}^{uj}(t)$。该机制利用人工势场原理实现智能体防碰撞。

*   **阶段二：最大加速度截断**
    考虑到机体动力系统的物理限制，修正后的加速度模长不得超过 $A_{max}$。采用如下缩放操作：
    $$
    \mathbf{a}_{u}(t) = A_{max} \cdot \tanh(\tilde{\mathbf{a}}_{u}(t))
    $$

*   **阶段三：速度更新与边界控制**
    基于一阶运动学模型更新速度，并强制执行最大速度约束 $V_{max}$：
    $$
    \mathbf{v}_{next} = \mathbf{v}_u(t) + \mathbf{a}_{u}(t) \cdot \tau_0
    $$
    $$
    \mathbf{v}_u(t+1) = \text{Clip}(\mathbf{v}_{next}, 0, V_{max}) = \begin{cases} 
    \mathbf{v}_{next}, & \|\mathbf{v}_{next}\| \le V_{max} \\
    V_{max} \cdot \frac{\mathbf{v}_{next}}{\|\mathbf{v}_{next}\|}, & \|\mathbf{v}_{next}\| > V_{max}
    \end{cases}
    $$
    这一级联过程确保了最终执行的飞行动作在位置、速度和加速度三个维度上均符合约束。

**2. 带宽资源归一化**

对于接入带宽分配系数 $\boldsymbol{\beta}_u(t) \in [0, 1]^{K_{max}}$，需满足总带宽约束 $\sum_{k} \beta_{k,u}(t) \le 1$。如 3.1 节所述，采用带掩码的 Softmax 函数处理：

$$
\beta_{k,u}(t) = \frac{e^{z_{bw, k}} \cdot m_k}{\sum_{j=1}^{K_{max}} e^{z_{bw, j}} \cdot m_j}
$$

该操作天然满足非负性与归一化约束，确保分配给所有用户的带宽之和严格等于总可用带宽。

**3. 能量安全层**

能量约束是制约 UAV 持续服务能力的关键因素（对应系统模型约束 C2）。虽然奖励函数中的惩罚项 $\Psi_{fail}$ 能引导智能体长期学习避免耗尽电量，但在训练初期，随机探索极易导致 UAV 因电量耗尽（$E_u(t) \le 0$）而使得 Episode 提前终止，严重阻碍了有效经验的收集。针对旋翼无人机特有的非线性“U”型功率特性（即在悬停和高速飞行时能耗均显著增加），设计了最大续航导向的安全修正机制。

*   **能耗估计：**
    在执行动作 $\mathbf{a}_{u}(t)$ 前，利用系统模型预计算下一时刻的剩余能量：
    $$
    \hat{E}_{u}(t+1) = E_u(t) - \left( P_u^{fly}(\|\tilde{\mathbf{v}}_u(t+1)\|) + P_{u}^{comm}(t) \right) \cdot \tau_0
    $$

*   **安全修正逻辑：**
    设定低电量警戒阈值 $E_{safe}$。若 $\hat{E}_{u}(t+1) < E_{safe}$，安全层将覆盖Actor的输出，将原本的随机探索动作修正为趋向最优经济航速 $v_{opt}$（对应功耗最低）的加速度指令。
    修正后的加速度 $\mathbf{a}_{safe}(t)$ 旨在调整当前速度 $\mathbf{v}_u(t)$ 使其模长逼近 $v_{opt}$，以最小化功率消耗 $P_u^{fly}$，从而最大化剩余生存时间以完成当前传输任务：

    $$
    \mathbf{a}_{u}^{final}(t) = \begin{cases} 
    \mathbf{a}_{u}(t), & \text{if } \hat{E}_{u}(t+1) \ge E_{safe} \\
    \kappa \cdot (v_{opt} - \|\mathbf{v}_u(t)\|) \cdot \frac{\mathbf{v}_u(t)}{\|\mathbf{v}_u(t)\|}, & \text{if } \hat{E}_{u}(t+1) < E_{safe}
    \end{cases}
    $$
    其中，$\kappa$ 为比例增益系数。该机制避免了 UAV 在低电量时因盲目加速或悬停导致的能量耗尽，解决了训练初期因能量耗尽导致的样本稀疏问题。

## 3.3 基于不确定性感知的优先级采样与梯度加权

在 SAGIN 这种高度动态且受限的环境中，不同时隙下的经验对策略优化的贡献显著不同。大多数平稳飞行状态属于平凡样本，提供的梯度信息受限；而卫星切换时刻、多普勒失锁临界点或用户突发业务等关键状态，包含极高的学习价值。常规的随机均匀采样难以捕捉稀疏但关键的边界状态，为了提升收敛效率，本节提出一种基于不确定性感知的优先级采样机制，在每一轮次采集的同步轨迹缓冲区内实施批次内重采样，实现在不引入分布失效风险前提下的高效学习。

### 3.3.1 样本信息价值度量 $\mathcal{I}_u(t)$

为了有效区分平凡样本与关键样本，定义标量指标 $\mathcal{I}_u(t)$ 来衡量第 $u$ 个智能体在时刻 $t$ 的经验元组 $\langle o_u, a_u, r_u, o'_u \rangle$ 的重要性。该指标由策略不确定性（策略熵）与价值偏差（GAE 绝对值）共同驱动。

**1. 归一化混合策略熵（Normalized Hybrid Entropy）**
策略熵反映了 Actor 网络对当前状态的困惑程度。高熵值意味着智能体位于探索不充分的区域。针对本系统的混合动作空间，将离散动作熵与连续动作微分熵分别归一化后进行加权融合：
$$
\tilde{\mathcal{H}}_u(t) = \lambda_{d} \cdot \frac{\mathcal{H}(\pi_{sat})}{\mathcal{H}_{max}^{d}} + \lambda_{c} \cdot \frac{\mathcal{H}(\pi_{cont})}{\mathcal{H}_{max}^{c}}
$$
其中，$\mathcal{H}(\pi_{sat})$ 为卫星关联的香农熵，$\mathcal{H}(\pi_{cont}) = \frac{1}{2} \log(2\pi e |\boldsymbol{\Sigma}|)$ 为连续动作的高斯微分熵。$\mathcal{H}_{max}^{d}$ 和 $\mathcal{H}_{max}^{c}$ 分别为滑动窗口内的最大观测熵值，用于消除量纲差异。$\lambda_d, \lambda_c$ 为权重系数，用于调节离散-连续动作空间对探索贡献的优先级。

**2. 价值偏差**
广义优势估计（GAE）$\hat{A}_u(t)$的绝对值反映了实际回报与 Critic 网络预期值的偏差程度。较大的优势值意味着当前状态的实际回报严重偏离了Critic网络的预期，通常对应拓扑剧变或约束触发。这类样本对于修正 Critic 网络更有价值。

**3. 综合信息价值定义**
最终的信息价值 $\mathcal{I}_u(t)$ 定义为：
$$
\mathcal{I}_u(t) = \eta \cdot \tilde{\mathcal{H}}_u(t) + (1-\eta) \cdot \frac{|\hat{A}_u(t)| + \epsilon}{\max_{\mathcal{B}}(|\hat{A}| + \epsilon)}
$$
其中，$\eta \in [0, 1]$ 为权衡探索需求与拟合误差的平衡系数，$\epsilon$ 为保障数值稳定性的小量，$\max_{\mathcal{B}}(\cdot)$ 表示当前采样批次内的最大值归一化。

### 3.3.2 主动采样与无偏性修正

基于信息价值 $\mathcal{I}_u(t)$，摒弃传统的均匀采样，设计了带有偏差修正的主动采样流程，迫使优化器在有限的训练步数内优先处理高价值样本。为了遵循 PPO 的在线策略限制，本算法不采用跨回合的深度经验回放，而是在每一轮次采集的同步缓冲区 $\mathcal{B}$ 内进行优先级采样。

**1. 优先级分配**
在完成一次轨迹采集后，计算当前缓冲区 $\mathcal{B}$ 内所有样本的 $\mathcal{I}_u(t)$，并将其转换为采样概率 $P(i) = \frac{(\mathcal{I}_i)^\alpha}{\sum_{k \in \mathcal{B}} (\mathcal{I}_k)^\alpha}$。其中 $\alpha$ 为调节优先程度的超参数（$\alpha=0$ 退化为均匀采样）。

**2. 小批次重采样**
在 PPO 进行 $K$ 个 Epoch 的参数更新时，依据 $P(i)$ 从 $\mathcal{B}$ 中抽取样本。通过提高临界事件（如多普勒偏移接近 $\nu_{max}$）的出现频率，迫使神经网络在有限的梯度步数内优先拟合困难场景的决策逻辑。

**3. 偏置修正与重要性权重**
优先级采样引入了分布偏置，需利用重要性采样权重 $w_i$ 对梯度进行修正，以保证目标函数的无偏性：
$$ 
w_i = \left( \frac{1}{|\mathcal{B}| \cdot P(i)} \right)^\beta 
$$
其中，$\beta$ 为退火系数，随训练进程从 $\beta_0$ 线性增加至 1。

**4. 修正后的 MAPPO 目标函数**
最终，将优先级权重 $w_i$ 整合至 MAPPO 的多目标优化架构中。设 $r_i(\theta)$ 为新旧策略概率比率，修正后的 Actor 代理损失函数 $L_{actor}(\theta)$ 与 Critic 价值损失函数 $L_{critic}(\phi)$ 分别表示为：
$$
L_{actor}(\theta) = \mathbb{E}_{i \sim P} \left[ w_i \cdot \left( L_i^{CLIP}(\theta) + c_e \cdot \tilde{\mathcal{H}}_i(\pi_\theta) \right) \right]
$$
其中，$L_i^{CLIP}(\theta) = \min \left( r_i(\theta) \hat{A}_i, \text{clip}(r_i(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_i \right)$ 为 PPO 的截断代理目标；$\tilde{\mathcal{H}}_i(\pi_\theta)$ 为当前策略的混合熵；$c_e$ 为控制探索强度的熵系数。

$$
L_{critic}(\phi) = \mathbb{E}_{i \sim P} \left[ w_i \cdot \frac{1}{2} (V_\phi(s_i) - V_i^{target})^2 \right]
$$
其中 $V_i^{target}$ 基于GAE目标值计算。

该机制通过在当前策略生成的样本空间内进行非均匀采样，使算法能够集中计算资源针对SAGIN中的关键状态进行处理，提升了算法在复杂强约束下的收敛性能。

## 3.4 课程学习引导的训练流程

在前文构建的 Dec-POMDP 模型中，智能体面临的是一个多目标、强耦合且高度动态的混合整数非线性规划（MINLP）问题。若在训练初期直接将其置于包含全物理约束（如多普勒失锁、非线性气动能耗）的真实环境中，智能体极易因频繁触发硬性约束（如电量耗尽导致 Episode 提前终止）而无法获得有效的奖励反馈，最终导致策略坍塌或陷入局部极小值。

为此，本研究引入课程学习（Curriculum Learning, CL）范式，采用约束逐步解禁的策略，将复杂的 SAGIN 协同计算任务分解为难度递增的三个阶段。该机制引导 Actor 网络首先掌握基础的空间几何特征，进而适应动态链路约束，最终实现端到端的能效全局优化。

### 3.4.1 阶段一：理想通信环境

**1. 约束松弛配置**
本阶段旨在让智能体快速掌握根据空间位置建立连接的能力，排除了复杂的物理层干扰。
*   **理想链路：** 保留卫星真实的轨道运动，但移除多普勒频移约束（即设 $\nu_{max} \to \infty$）。只要卫星在几何视距内（$\theta > \theta_{min}$），链路即视为完美连通，只考虑路径损耗。
*   **无限能源：** 锁定 UAV 电量为恒定值（$E_u \equiv 100\%$），关闭能耗惩罚 $\Psi_{fail}^{batt}$ 和 3.2.2 节的能量安全层。
*   **静态用户：** 地面用户位置固定，仅产生恒定速率的任务流，降低环境的随机性。

**2. 训练目标**
*   **空间感知：** 训练 Actor 网络建立从局部观测 $o_u$ 到空间机动 $\mathbf{a}_{acc}$ 的映射，学会向任务积压量 $Q_k$ 大的热点区域聚集。
*   **基础关联：** 初步掌握多星覆盖下的负载均衡，即在可见卫星中选择几何距离最近的目标。
*   **效果：** 由于不存在能源耗尽和通信中断风险，智能体能迅速获得基于吞吐量的高额奖励，收敛至几何位置上的贪婪最优解。

### 3.4.2 阶段二：动态拓扑与链路鲁棒性适应

**1. 约束注入配置**
在阶段一策略参数的基础上，引入通信层面的动态约束，训练智能体对链路质量的敏感度。
*   **激活多普勒约束：** 恢复多普勒频移限制 $|\nu_{u,l}| \le \nu_{max}$。此时，3.2.1 节设计的动作掩码 $\mathbf{M}_{sat}$ 生效。若智能体选择了相对速度过大的卫星，将导致连接失败并受到惩罚。
*   **动态信道：** 启用真实的信道衰落模型，信噪比 $\text{SNR}_{u,l}^{eff}$ 随距离和干扰动态波动。
*   **动态用户流：** 地面用户任务到达服从非平稳泊松过程，增加了负载的不确定性。

**2. 训练目标**
*   **特征关联学习：** 迫使 Attention 模块（尤其是卫星编码器）关注 $\nu_{u,l}$ 和 $\text{SNR}$ 信道质量特征，而不仅仅是几何距离。
*   **主动切换机制：** 策略网络需能够预估卫星可用性，即在当前卫星即将飞出视线或多普勒频移即将超限之前，主动断开连接并平滑切换至下一颗最优卫星，以保障回程链路的连续性。

### 3.4.3 阶段三：全约束下的能效-性能权衡

**1. 真实物理环境配置**
引入所有物理约束，训练模型追求时延与能耗的平衡。
*   **非线性气动能耗：** 启用基于旋翼空气动力学的U 型功率模型 $P_u^{fly}(\|\mathbf{v}\|)$。
*   **生存约束介入：** 恢复电池耗尽惩罚，且 3.2.2 节中的能量安全层生效。若智能体盲目加速，将被强制修正动作以维持生存。
*   **完整奖励函数：** 启用包含时延、能耗、切换代价及生存惩罚的全目标奖励函数。

**2. 训练目标**
智能体需要在服务质量与能耗之间寻找平衡。
*   **速度控制权衡：** 智能体需学会在高速飞行以提升吞吐与低速飞行以减少能耗之间博弈，修正阶段一形成的越快到达目标位置越好的先验。
*   **长期生存规划：** 学习在低电量状态下优先保障自身能量安全，实现全任务周期内的累积回报最大化。

## 3.5 算法复杂度与扩展性分析

在 6G SAGIN 场景中，算法的计算复杂度直接决定了其能否在毫秒级时隙内完成实时决策。鉴于本研究采用集中式训练-分布式执行（CTDE）架构，我们分别对部署在 UAV 机载端的 Actor 网络（在线推理）和部署在中心服务器的 Critic 网络及训练流程（离线训练）进行时空复杂度分析。

### 3.5.1 在线推理复杂度（分布式 Actor）

在执行阶段，每个 UAV 需根据局部观测 $o_u(t)$ 独立生成动作。计算开销主要来源于特征提取器中的目标注意力机制与时序聚合模块。

设 UAV 局部观测范围内的实体总数上限为 $N_{local} = |\mathcal{K}_{u}| + |\mathcal{O}_{u}^{sats}| + |\mathcal{N}_{u}|$，网络隐藏层维度为 $D$。

1.  **注意力特征提取：**
    如 3.1.2 节所述，Actor 网络采用目标注意力机制，以 UAV 自身特征（$1 \times D$）为 Query，环境实体特征（$N_{local} \times D$）为 Key 和 Value。主要计算步骤分解如下：
    *   **线性投影：** 将输入特征映射为 $Q, K, V$ 矩阵。计算量为 $3 \times (N_{local} \times D^2)$，复杂度为 $O(N_{local} \cdot D^2)$。
    *   **注意力分数计算：** 计算 $QK^T$，维度变化为 $(1 \times D) \times (D \times N_{local}) \to (1 \times N_{local})$。复杂度为 $O(N_{local} \cdot D)$。
    *   **加权聚合：** 计算 Score $\times V$，维度变化为 $(1 \times N_{local}) \times (N_{local} \cdot D) \to (1 \times D)$。复杂度为 $O(N_{local} \cdot D)$。

    **结论：** Actor 注意力模块的总时间复杂度为 $O(N_{local} \cdot D^2)$。该复杂度随观测节点数量 $N_{local}$ 呈线性增长，而非标准自注意力机制的平方级增长。这确保了在密集用户或多星覆盖场景下，机载计算开销始终可控。

2.  **时序记忆与决策（GRU & MLP）：**
    *   GRU 单元仅处理聚合后的固定维度上下文向量，复杂度为 $O(D^2)$，与输入节点数量解耦。
    *   MLP 输出层与动作掩码均为向量级点运算，复杂度为 $O(D)$。

综上，单个 UAV 的单步推理时间复杂度为 $O(N_{local} \cdot D^2 + D^2)$。鉴于 $D$ 为固定常数，算法运行时间与邻居节点数线性相关，可满足 UAV 嵌入式处理器对低延迟的严苛要求。

### 3.5.2 离线训练复杂度（集中式 Critic 与优先级采样）

训练阶段在具有高算力集群的中心服务器上进行，主要开销包括全局状态的自注意力评估以及基于不确定性的批次内重采样。

1.  **全局价值评估：**
    Critic 网络需处理全网全局状态，设全网总节点数为 $N_{total}$（$N_{total} \gg N_{local}$）。
    为了捕捉复杂的全网拓扑依赖，Critic 采用多头自注意力机制。
    *   **自注意力矩阵计算：** Query 和 Key 均来自全网所有节点，计算 $QK^T$ 涉及 $(N_{total} \times D) \times (D \times N_{total})$ 的矩阵乘法。
    *   **复杂度：** $O(N_{total}^2 \cdot D)$。Critic 网络的计算量随全网规模呈平方增长。尽管开销较大，但考虑到训练过程是离线进行的，地面站充足的计算资源可有效覆盖该开销。

2.  **批次内优先级采样维护：**
    根据 3.3 节所述，算法在每一轮轨迹采集后维护一个大小为 $|\mathcal{B}|$ 的同步缓冲区。
    *   **价值度量计算：** 遍历缓冲区计算所有样本的综合信息价值 $\mathcal{I}_u(t)$，复杂度为 $O(|\mathcal{B}| \cdot D)$。
    *   **重采样与权重计算：** 在 $K$ 个更新 Epoch 中，依据 $P(i)$ 进行小批次抽取，大小为$B$。由于 $|\mathcal{B}|$ 规模远小于传统经验回放池，通过简单的累积分布函数（CDF）采样即可在 $O(|\mathcal{B}|)$ 复杂度内完成，或使用分段树优化至 $O(B \cdot \log |\mathcal{B}|)$。
    *   **结论：** 相比于随机均匀采样，优先级采样仅引入了极低的线性计算增量，但通过聚焦关键状态，显著降低了达到模型收敛所需的总训练迭代次数。

### 3.5.3 空间复杂度与扩展性分析

1.  **参数共享机制：**
    得益于同构多智能体架构，全网所有 UAV 共享同一套 Actor 网络参数 $\theta$。模型参数总量 $M_{\theta}$ 仅与网络层数和隐藏层维度 $D$ 有关，不随 UAV 数量 $U$ 的增加而增长，即空间复杂度为 $O(1)$（相对于智能体数量）。这使得算法能够扩展至包含更多无人机的集群网络，而不会导致显存溢出。

2.  **拓扑自适应与零样本迁移：**
    传统的基于 MLP 的强化学习算法要求输入维度固定，一旦网络拓扑改变（如增加卫星或用户），必须重构网络并从头训练。
    本研究引入的注意力机制具有置换不变性和变长输入处理能力。这意味着训练完成后的模型无需重新架构或重新训练，即可直接部署于节点数量动态变化、卫星过顶频率不同、甚至节点密度存在数量级差异的新场景。这种强大的拓扑自适应性极大增强了算法在真实动态 SAGIN 环境中的工程实用价值。